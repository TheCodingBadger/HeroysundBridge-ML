{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score as score\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Suppress tensorflow logging due to un-relevant warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = 19000\n",
    "validation_split = 4750\n",
    "test_split = 2640 \n",
    "\n",
    "asset_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML-Assets'))\n",
    "df = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "df.index = pd.to_datetime(df['Date'], format='%Y%m%d%H')\n",
    "df = df['2020-10-01 00:00:00':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Prediction (t + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: isbn = {1492078190}, title = {AI and Machine Learning for Coders: A Programmer's Guide to Artificial Intelligence}, author = {Moroney, Laurence}, Page(s): 169-171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = 1\n",
    "test_df = df[['Point_6_S_mean']][(training_split + validation_split):]\n",
    "\n",
    "naive_df = df[['Point_6_S_mean']][((training_split + validation_split) - delay):-delay]\n",
    "print(naive_df.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_df.index, test_df['Point_6_S_mean'], label='Test Dataset')\n",
    "plt.plot(test_df.index, naive_df['Point_6_S_mean'], label='Naive Prediction')\n",
    "plt.xlabel('Date', fontsize=16)\n",
    "plt.ylabel('Point_6_S_mean', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(test_df['Point_6_S_mean'], naive_df['Point_6_S_mean']).numpy()}\")\n",
    "print(f\"MAE: {mean_absolute_error(test_df['Point_6_S_mean'], naive_df['Point_6_S_mean']).numpy()}\")\n",
    "print(f\"R2: {score(test_df['Point_6_S_mean'], naive_df['Point_6_S_mean'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sckit-learn linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bare eksponert på test settet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_df = df[-test_split:]\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = lin_reg_df[['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]\n",
    "y = lin_reg_df['Point_1_N_mean']\n",
    "\n",
    "X_test = lin_reg_df[['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]#'Average_Global_Radiation_(1h)',\n",
    "y_test = lin_reg_df['Point_1_N_mean']\n",
    "\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(lin_reg_df.index, y_test, label='Test Dataset')\n",
    "plt.plot(lin_reg_df.index, predictions, label='Linear Regression Prediction')\n",
    "plt.xlabel('Date', fontsize=16*1.5)\n",
    "plt.ylabel(\"'Point_1_N_mean', Strain [μm/m]\", fontsize=16*1.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14*1.5)\n",
    "plt.xticks(fontsize=14*1.5)\n",
    "plt.legend(fontsize=14*1.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, predictions)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, predictions)}\")\n",
    "print(f\"R^2: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_df = df[-test_split:]\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = lin_reg_df[['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]\n",
    "y = lin_reg_df['Point_6_S_mean']\n",
    "\n",
    "X_test = lin_reg_df[['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]#'Average_Global_Radiation_(1h)',\n",
    "y_test = lin_reg_df['Point_6_S_mean']\n",
    "\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(lin_reg_df.index, y_test, label='Test Dataset')\n",
    "plt.plot(lin_reg_df.index, predictions, label='Linear Regression Prediction')\n",
    "plt.xlabel('Date', fontsize=16*1.5)\n",
    "plt.ylabel(\"'Point_6_S_mean', Strain [μm/m]\", fontsize=16*1.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14*1.5)\n",
    "plt.xticks(fontsize=14*1.5)\n",
    "plt.legend(fontsize=14*1.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, predictions)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, predictions)}\")\n",
    "print(f\"R^2: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trent på data før test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_df1 = df[:-test_split]\n",
    "lin_reg_df2 = df[-test_split:]\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = lin_reg_df1[['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]\n",
    "y = lin_reg_df1['Point_1_N_mean']\n",
    "\n",
    "X_test = lin_reg_df2[['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]#'Average_Global_Radiation_(1h)',\n",
    "y_test = lin_reg_df2['Point_1_N_mean']\n",
    "\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(lin_reg_df.index, y_test, label='Test Dataset')\n",
    "plt.plot(lin_reg_df.index, predictions, label='Linear Regression Prediction')\n",
    "plt.xlabel('Date', fontsize=16*1.5)\n",
    "plt.ylabel(\"'Point_1_N_mean', Strain [μm/m]\", fontsize=16*1.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14*1.5)\n",
    "plt.xticks(fontsize=14*1.5)\n",
    "plt.legend(fontsize=14*1.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, predictions)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, predictions)}\")\n",
    "print(f\"R^2: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_df1 = df[:-test_split]\n",
    "lin_reg_df2 = df[-test_split:]\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = lin_reg_df1[['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]\n",
    "y = lin_reg_df1['Point_6_S_mean']\n",
    "\n",
    "X_test = lin_reg_df2[['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]#'Average_Global_Radiation_(1h)',\n",
    "y_test = lin_reg_df2['Point_6_S_mean']\n",
    "\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(lin_reg_df.index, y_test, label='Test Dataset')\n",
    "plt.plot(lin_reg_df.index, predictions, label='Linear Regression Prediction')\n",
    "plt.xlabel('Date', fontsize=16*1.5)\n",
    "plt.ylabel(\"'Point_6_S_mean', Strain [μm/m]\", fontsize=16*1.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14*1.5)\n",
    "plt.xticks(fontsize=14*1.5)\n",
    "plt.legend(fontsize=14*1.5)\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, predictions)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, predictions)}\")\n",
    "print(f\"R^2: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24 * 30\n",
    "\n",
    "moving_avg = df[(training_split + validation_split):][['Point_6_S_mean']].rolling(window=window, min_periods=1).mean().dropna()\n",
    "test_df = df[(training_split + validation_split):][['Point_6_S_mean']].dropna()\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(test_df.index, test_df['Point_6_S_mean'], label='Test Dataset')\n",
    "plt.plot(moving_avg.index, moving_avg['Point_6_S_mean'], label='Moving Average')\n",
    "plt.xlabel('Date', fontsize=16*1.5)\n",
    "plt.ylabel(\"'Point_6_S_mean' Strain [μm/m]\", fontsize=16*1.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14*1.5)\n",
    "plt.xticks(fontsize=14*1.5)\n",
    "plt.legend(fontsize=14*1.5)\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(test_df['Point_6_S_mean'], moving_avg['Point_6_S_mean'].to_numpy())\n",
    "mae = mean_absolute_error(test_df['Point_6_S_mean'], moving_avg['Point_6_S_mean'].to_numpy())\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {score(test_df['Point_6_S_mean'], moving_avg['Point_6_S_mean'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24 * 30\n",
    "\n",
    "moving_avg = df[(training_split + validation_split):][['Point_1_N_mean']].rolling(window=window, min_periods=1).mean().dropna()\n",
    "test_df = df[(training_split + validation_split):][['Point_1_N_mean']].dropna()\n",
    "\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.plot(test_df.index, test_df['Point_1_N_mean'], label='Test Dataset')\n",
    "plt.plot(moving_avg.index, moving_avg['Point_1_N_mean'], label='Moving Average')\n",
    "plt.xlabel('Date', fontsize=16*1.5)\n",
    "plt.ylabel(\"'Point_1_N_mean' Strain [μm/m]\", fontsize=16*1.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14*1.5)\n",
    "plt.xticks(fontsize=14*1.5)\n",
    "plt.legend(fontsize=14*1.5)\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(test_df['Point_1_N_mean'], moving_avg['Point_1_N_mean'].to_numpy())\n",
    "mae = mean_absolute_error(test_df['Point_1_N_mean'], moving_avg['Point_1_N_mean'].to_numpy())\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R2: {score(test_df['Point_1_N_mean'], moving_avg['Point_1_N_mean'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html#statsmodels.tsa.seasonal.seasonal_decompose\n",
    "\n",
    "(Copilot-generated description!!!) Statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24*30\n",
    "window_halved = window // 2\n",
    "\n",
    "# Perform seasonal decomposition on the test dataset\n",
    "test_df = df[(training_split + validation_split):][['Point_6_S_mean']]  # test_df = df[['Point_6_S_mean']]\n",
    "seasonal_df = seasonal_decompose(test_df, model='additive', period=window)\n",
    "fig = seasonal_df.plot()\n",
    "fig.set_size_inches((18, 14))\n",
    "\n",
    "# Adjust font size of tick labels and y-axis labels for all subplots\n",
    "for ax in fig.get_axes():\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14*1.5)  # adjust tick font size\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=14*1.5)  # adjust y-axis label font size\n",
    "\n",
    "# Adjust x-axis label only for the last subplot\n",
    "fig.get_axes()[-1].set_xlabel('Date', fontsize=14*1.5)\n",
    "fig.get_axes()[0].set_ylabel(\"Strain [μm/m]\", fontsize=14*1.5)\n",
    "fig.get_axes()[1].set_ylabel(\"Trend, Strain [μm/m]\", fontsize=14*1.5)\n",
    "fig.get_axes()[2].set_ylabel(\"Seasonality [μm/m]\", fontsize=14*1.5)\n",
    "fig.get_axes()[3].set_ylabel(\"Residual [μm/m]\", fontsize=14*1.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract the trend component from the seasonal decomposition\n",
    "seasonal_trend_df = pd.DataFrame(seasonal_df.trend)\n",
    "seasonal_trend_df = seasonal_trend_df.dropna()\n",
    "test_df = test_df[(window//2):-(window//2)]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(seasonal_trend_df.index, test_df, label='Reduced Test Dataset')  # Plotting 'Point_1_N_mean' against the index of seasonal_trend_df\n",
    "# plt.plot(seasonal_trend_df.index, seasonal_trend_df, label='Trend Component')\n",
    "# plt.xlabel('Date', fontsize=16)\n",
    "# plt.ylabel(\"'Point_6_S_mean' Strain [μm/m]\", fontsize=16)\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.yticks(fontsize=14)\n",
    "# plt.xticks(fontsize=14)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(test_df['Point_6_S_mean'], seasonal_trend_df['trend'])}\")\n",
    "print(f\"MAE: {mean_absolute_error(test_df['Point_6_S_mean'], seasonal_trend_df['trend'])}\")\n",
    "print(f\"R2: {score(test_df['Point_6_S_mean'], seasonal_trend_df['trend'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24*30\n",
    "window_halved = window // 2\n",
    "\n",
    "# Perform seasonal decomposition on the test dataset\n",
    "test_df = df[(training_split + validation_split):][['Point_1_N_mean']]  # test_df = df[['Point_6_S_mean']]\n",
    "seasonal_df = seasonal_decompose(test_df, model='additive', period=window)\n",
    "fig = seasonal_df.plot()\n",
    "fig.set_size_inches((18, 14))\n",
    "\n",
    "# Adjust font size of tick labels and y-axis labels for all subplots\n",
    "for ax in fig.get_axes():\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14*1.5)  # adjust tick font size\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=14*1.5)  # adjust y-axis label font size\n",
    "\n",
    "# Adjust x-axis label only for the last subplot\n",
    "fig.get_axes()[-1].set_xlabel('Date', fontsize=14*1.5)\n",
    "fig.get_axes()[0].set_ylabel(\"Strain [μm/m]\", fontsize=14*1.5)\n",
    "fig.get_axes()[1].set_ylabel(\"Trend, Strain [μm/m]\", fontsize=14*1.5)\n",
    "fig.get_axes()[2].set_ylabel(\"Seasonality [μm/m]\", fontsize=14*1.5)\n",
    "fig.get_axes()[3].set_ylabel(\"Residual [μm/m]\", fontsize=14*1.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract the trend component from the seasonal decomposition\n",
    "seasonal_trend_df = pd.DataFrame(seasonal_df.trend)\n",
    "seasonal_trend_df = seasonal_trend_df.dropna()\n",
    "test_df = test_df[(window//2):-(window//2)]\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(seasonal_trend_df.index, test_df, label='Reduced Test Dataset')  # Plotting 'Point_1_N_mean' against the index of seasonal_trend_df\n",
    "# plt.plot(seasonal_trend_df.index, seasonal_trend_df, label='Trend Component')\n",
    "# plt.xlabel('Date', fontsize=16)\n",
    "# plt.ylabel(\"'Point_6_S_mean' Strain [μm/m]\", fontsize=16)\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.yticks(fontsize=14)\n",
    "# plt.xticks(fontsize=14)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(test_df['Point_1_N_mean'], seasonal_trend_df['trend'])}\")\n",
    "print(f\"MAE: {mean_absolute_error(test_df['Point_1_N_mean'], seasonal_trend_df['trend'])}\")\n",
    "print(f\"R2: {score(test_df['Point_1_N_mean'], seasonal_trend_df['trend'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24*30\n",
    "window_halved = window // 2\n",
    "\n",
    "# Perform seasonal decomposition on the test dataset\n",
    "test_df = df[(training_split + validation_split):][['Point_1_N_mean']]\n",
    "seasonal_df = seasonal_decompose(test_df, model='additive', period=window)\n",
    "seasonal_df.plot()\n",
    "\n",
    "# Extract the trend component from the seasonal decomposition\n",
    "seasonal_trend_df = pd.DataFrame(seasonal_df.trend)\n",
    "seasonal_trend_df = seasonal_trend_df.dropna()\n",
    "test_df = test_df[(window//2):-(window//2)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(seasonal_trend_df.index, test_df, label='Reduced Test Dataset')  # Plotting 'Point_1_N_mean' against the index of seasonal_trend_df\n",
    "plt.plot(seasonal_trend_df.index, seasonal_trend_df, label='Trend Component')\n",
    "plt.xlabel('Date', fontsize=16)\n",
    "plt.ylabel(\"'Point_1_N_mean' Strain [μm/m]\", fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(test_df['Point_1_N_mean'], seasonal_trend_df['trend'])}\")\n",
    "print(f\"MAE: {mean_absolute_error(test_df['Point_1_N_mean'], seasonal_trend_df['trend'])}\")\n",
    "print(f\"R2: {score(test_df['Point_1_N_mean'], seasonal_trend_df['trend'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented Dickey-Fuller test + KPSS test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(GitHub Copilot) {\n",
    "The KPSS and ADF tests should be applied to the dataset that you are using to train your machine learning model. This is because the purpose of these tests is to help you understand the underlying properties of your data so that you can appropriately preprocess the data and select a suitable model. If your training data is non-stationary, the model you train on this data may not perform well on the test data, even if the test data is from the same non-stationary distribution. This is because many machine learning models, especially linear models and time series models, assume that the data is stationary. Applying the tests to the test set would not be as useful, because you do not use the test set to train your model. The test set is used to evaluate the performance of the model that was trained on the training set. However, if the test set is from a different distribution than the training set, this could indicate a problem with your data splitting method. In summary, apply the KPSS and ADF tests to your training data to check for stationarity before training your model. If the tests indicate that your data is non-stationary, you may need to transform your data to make it stationary before training your model.}\n",
    "\n",
    "Relevant Article: https://medium.com/@tannyasharma21/comparision-study-of-adf-vs-kpss-test-c9d8dec4f62a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/\n",
    "\n",
    "The Augmented Dickey-Fuller Test is used to determine if time-series data is stationary or not. Similar to a t-test, we set a significance level before the test and make conclusions on the hypothesis based on the resulting p-value. \"Another point to remember is the ADF test is fundamentally a statistical significance test\" - Selva Prabhakaran (https://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.machinelearningplus.com/time-series/kpss-test-for-stationarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Info!) This cell is heavily influenced by GitHub Copilot\n",
    "\n",
    "# ADF Test\n",
    "print('Results of Augmented Dickey-Fuller Test (ADF):')\n",
    "dftest_adf = adfuller(df['Point_1_N_mean'], autolag='AIC')  # Choosing data that ML model has seen (Excluding test data) # [:(validation_split + test_split)]\n",
    "\n",
    "# Create a Series to hold the ADF test results\n",
    "adf_output = pd.Series(dftest_adf[0:4], index=['Test Statistic (ADF)', 'p-value (ADF)', '#Lags Used (ADF)', 'Number of Observations Used (ADF)'])\n",
    "\n",
    "# Add critical values to the Series\n",
    "for key, value in dftest_adf[4].items():\n",
    "    adf_output[f'Critical Value ({key}) (ADF)'] = value\n",
    "\n",
    "print(adf_output)\n",
    "\n",
    "# Determine if we reject the null hypothesis based on a significance level of 0.05\n",
    "if adf_output['p-value (ADF)'] < 0.05:\n",
    "    print(\"Reject the null hypothesis (H0) - Data is stationary (ADF)\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0) - Data is non-stationary (ADF)\")\n",
    "\n",
    "# KPSS Test\n",
    "print('\\nResults of KPSS Test:')\n",
    "kpsstest = kpss(df['Point_1_N_mean'], regression='ct', nlags='auto') #[:(validation_split + test_split)]\n",
    "\n",
    "# Create a Series to hold the KPSS test results\n",
    "kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic (KPSS)', 'p-value (KPSS)', 'Lags Used (KPSS)'])\n",
    "\n",
    "# Add critical values to the Series\n",
    "for key, value in kpsstest[3].items():\n",
    "    kpss_output[f'Critical Value ({key}) (KPSS)'] = value\n",
    "\n",
    "print(kpss_output)\n",
    "\n",
    "# Determine if we reject the null hypothesis based on a significance level of 0.05\n",
    "if kpss_output['p-value (KPSS)'] < 0.05:\n",
    "    print(\"Reject the null hypothesis (H0) - Data is non-stationary (KPSS)\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0) - Data is stationary (KPSS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Info!) This cell is heavily influenced by GitHub Copilot\n",
    "\n",
    "# ADF Test\n",
    "print('Results of Augmented Dickey-Fuller Test (ADF):')\n",
    "dftest_adf = adfuller(df['Point_6_S_mean'], autolag='AIC')  # Choosing data that ML model has seen (Excluding test data)\n",
    "\n",
    "# Create a Series to hold the ADF test results\n",
    "adf_output = pd.Series(dftest_adf[0:4], index=['Test Statistic (ADF)', 'p-value (ADF)', '#Lags Used (ADF)', 'Number of Observations Used (ADF)'])\n",
    "\n",
    "# Add critical values to the Series\n",
    "for key, value in dftest_adf[4].items():\n",
    "    adf_output[f'Critical Value ({key}) (ADF)'] = value\n",
    "\n",
    "print(adf_output)\n",
    "\n",
    "# Determine if we reject the null hypothesis based on a significance level of 0.05\n",
    "if adf_output['p-value (ADF)'] < 0.05:\n",
    "    print(\"Reject the null hypothesis (H0) - Data is stationary (ADF)\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0) - Data is non-stationary (ADF)\")\n",
    "\n",
    "# KPSS Test\n",
    "print('\\nResults of KPSS Test:')\n",
    "kpsstest = kpss(df['Point_6_S_mean'], regression='ct', nlags='auto')\n",
    "\n",
    "# Create a Series to hold the KPSS test results\n",
    "kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic (KPSS)', 'p-value (KPSS)', 'Lags Used (KPSS)'])\n",
    "\n",
    "# Add critical values to the Series\n",
    "for key, value in kpsstest[3].items():\n",
    "    kpss_output[f'Critical Value ({key}) (KPSS)'] = value\n",
    "\n",
    "print(kpss_output)\n",
    "\n",
    "# Determine if we reject the null hypothesis based on a significance level of 0.05\n",
    "if kpss_output['p-value (KPSS)'] < 0.05:\n",
    "    print(\"Reject the null hypothesis (H0) - Data is non-stationary (KPSS)\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0) - Data is stationary (KPSS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX  (Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX\n",
    "\n",
    "https://towardsdatascience.com/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6\n",
    "\n",
    "https://pypi.org/project/pmdarima/\n",
    "\n",
    "(Copilot-generated description!!!) SARIMAX is an acronym for Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model. It is a class of models that explains a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0 # Varaible to shift how big of training+vaildation set we want to use\n",
    "\n",
    "data = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "data.index = pd.to_datetime(data['Date'], format='%Y%m%d%H').dropna()\n",
    "data = data[['Point_1_N_mean', 'Average_Global_Radiation_(1h)','PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']][a:-test_split]  #['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']\n",
    "data['month'] = data.index.month\n",
    "data['hour'] = data.index.hour\n",
    "data['day'] = data.index.dayofyear\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(\n",
    "    data['Point_1_N_mean'],  # The time series to which the ARIMA model is fit.\n",
    "    exogenous=data[['month', 'hour', 'day', 'Average_Global_Radiation_(1h)','PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']],  # The external variables to include in the model.\n",
    "    start_p=1,  # This is the order of the AR term.\n",
    "    start_q=1,  # This is the order of the MA term.\n",
    "    test='adf',  # The type of unit root test to use in order to determine the order of differencing.\n",
    "    max_p=3,  # The maximum value of p to try when fitting the model.\n",
    "    max_q=3,  # The maximum value of q to try when fitting the model.\n",
    "    m=7,  # The number of periods in each season. This affects the seasonal differencing.\n",
    "    start_P=0,  # The starting value of P in auto_arima. This is the order of the seasonal AR term.\n",
    "    seasonal=True,  # Whether to include seasonal differencing in the model.\n",
    "    d=None,  # The order of first-differencing. If None, the value is automatically determined.\n",
    "    D=1,  # The order of seasonal differencing.\n",
    "    trace=True,  # Whether to print status on the fits.\n",
    "    error_action='ignore',  # If a model cannot be fit, ignore the error and continue.\n",
    "    suppress_warnings=True,  # If True, do not print warnings.\n",
    "    stepwise=True,  # If True, use the stepwise algorithm to fit the model.\n",
    "    maxiter=15  # The maximum number of function evaluations.\n",
    ")\n",
    "\n",
    "model.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code is reused from the following code: https://gist.githubusercontent.com/brendanartley/c69185f28e678c7221546a9c43825ec0/raw/d702ee2d1e4ad57e4ea9f92e5fa09a3414b509e8/gistfile1.txt\n",
    "Reference: https://towardsdatascience.com/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, periods=test_split):\n",
    "    forecast, conf_int = model.predict(n_periods=periods, return_conf_int=True)\n",
    "    \n",
    "    forecast_index = pd.date_range(start=data.index[-1], periods=periods + 1, freq='H')[1:]\n",
    "    lookback_variable = 500*3\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    plt.plot(df.index[(training_split+validation_split)-lookback_variable:-test_split], df['Point_1_N_mean'].iloc[(training_split+validation_split)-lookback_variable:-test_split], label='Training and Validation Dataset', color='black')\n",
    "    plt.plot(df[-test_split:].index, df['Point_1_N_mean'][-test_split:], label='Test Dataset', color='blue')\n",
    "    plt.plot(forecast_index, forecast, label='SARIMAx Forecast', color='orange')\n",
    "    plt.fill_between(forecast_index, conf_int[:, 0], conf_int[:, 1], color='k', alpha=0.2) # Between the confidence intervals\n",
    "    plt.xlabel(\"Date\", fontsize=16*1.5)\n",
    "    plt.ylabel(\"'Point_1_N_mean' Strain [μm/m]\", fontsize=16*1.5)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(fontsize=14*1.5)\n",
    "    plt.xticks(fontsize=14*1.5)\n",
    "    plt.legend(fontsize=14*1.5)\n",
    "    plt.show()\n",
    "\n",
    "    test_df = df[-test_split:][['Point_1_N_mean']]\n",
    "    forecast_values = forecast.to_numpy().flatten()\n",
    "    test_values = test_df.to_numpy().flatten()\n",
    "   \n",
    "    print(f\"MSE: {mean_squared_error(test_values, forecast_values)}\")\n",
    "    print(f\"MAE: {mean_absolute_error(test_values, forecast_values)}\")\n",
    "    print(f\"R2: {score(test_values, forecast_values)}\")\n",
    "\n",
    "forecast(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For pointS6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0 # Varaible to shift how big of training+vaildation set we want to use\n",
    "\n",
    "data = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "data.index = pd.to_datetime(data['Date'], format='%Y%m%d%H').dropna()\n",
    "data = data[['Point_6_S_mean', 'Average_Global_Radiation_(1h)','PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']][a:-test_split]  #['PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']\n",
    "data['month'] = data.index.month\n",
    "data['hour'] = data.index.hour\n",
    "data['day'] = data.index.dayofyear\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(\n",
    "    data['Point_6_S_mean'],  # The time series to which the ARIMA model is fit.\n",
    "    exogenous=data[['month', 'hour', 'day', 'Average_Global_Radiation_(1h)','PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']],  # The external variables to include in the model.\n",
    "    start_p=1,  # This is the order of the AR term.\n",
    "    start_q=1,  # This is the order of the MA term.\n",
    "    test='adf',  # The type of unit root test to use in order to determine the order of differencing.\n",
    "    max_p=3,  # The maximum value of p to try when fitting the model.\n",
    "    max_q=3,  # The maximum value of q to try when fitting the model.\n",
    "    m=7,  # The number of periods in each season. This affects the seasonal differencing.\n",
    "    start_P=0,  # The starting value of P in auto_arima. This is the order of the seasonal AR term.\n",
    "    seasonal=True,  # Whether to include seasonal differencing in the model.\n",
    "    d=None,  # The order of first-differencing. If None, the value is automatically determined.\n",
    "    D=1,  # The order of seasonal differencing.\n",
    "    trace=True,  # Whether to print status on the fits.\n",
    "    error_action='ignore',  # If a model cannot be fit, ignore the error and continue.\n",
    "    suppress_warnings=True,  # If True, do not print warnings.\n",
    "    stepwise=True,  # If True, use the stepwise algorithm to fit the model.\n",
    "    maxiter=15  # The maximum number of function evaluations.\n",
    ")\n",
    "\n",
    "model.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, periods=test_split):\n",
    "    forecast, conf_int = model.predict(n_periods=periods, return_conf_int=True)\n",
    "    \n",
    "    forecast_index = pd.date_range(start=data.index[-1], periods=periods + 1, freq='H')[1:]\n",
    "    lookback_variable = 500*3\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    plt.plot(df.index[(training_split+validation_split)-lookback_variable:-test_split], df['Point_6_S_mean'].iloc[(training_split+validation_split)-lookback_variable:-test_split], label='Training and Validation Dataset', color='black')\n",
    "    plt.plot(df[-test_split:].index, df['Point_6_S_mean'][-test_split:], label='Test Dataset', color='blue')\n",
    "    plt.plot(forecast_index, forecast, label='SARIMAx Forecast', color='orange')\n",
    "    plt.fill_between(forecast_index, conf_int[:, 0], conf_int[:, 1], color='k', alpha=0.2) # Between the confidence intervals\n",
    "    plt.xlabel(\"Date\", fontsize=16*1.5)\n",
    "    plt.ylabel(\"'Point_6_S_mean' Strain [μm/m]\", fontsize=16*1.5)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(fontsize=14*1.5)\n",
    "    plt.xticks(fontsize=14*1.5)\n",
    "    plt.legend(fontsize=14*1.5)\n",
    "    plt.show()\n",
    "\n",
    "    test_df = df[-test_split:][['Point_6_S_mean']]\n",
    "    forecast_values = forecast.to_numpy().flatten()\n",
    "    test_values = test_df.to_numpy().flatten()\n",
    "   \n",
    "    print(f\"MSE: {mean_squared_error(test_values, forecast_values)}\")\n",
    "    print(f\"MAE: {mean_absolute_error(test_values, forecast_values)}\")\n",
    "    print(f\"R2: {score(test_values, forecast_values)}\")\n",
    "\n",
    "forecast(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section for figures in baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "  \"calories\": [420, 380, 390, 420, 380, 390, 420*1.2, 380*1.6, 390*1.4, 420*2.3]\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data)\n",
    "df2 = df1.shift(-1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df2['calories'][:-2], label='actual') \n",
    "plt.plot(df1['calories'][1:-1], label='t + 1')\n",
    "plt.xlabel('x-axis', fontsize=14)\n",
    "plt.ylabel('y-axis', fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "  \"calories\": [420, 380, 390, 420, 380, 390]\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data)\n",
    "df2 = df1.shift(-1)\n",
    "\n",
    "# Shift the 't + 1' series one step ahead\n",
    "t_plus_1 = df1['calories'][1:].reset_index(drop=True)\n",
    "\n",
    "# Concatenate 'actual' and 'None' value\n",
    "actual = pd.concat([df2['calories'].dropna(), pd.Series([None])]).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(actual, label='actual') \n",
    "plt.plot(t_plus_1, label='t + 1')\n",
    "plt.xlabel('Time Step', fontsize=14)\n",
    "plt.ylabel('Calories', fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
