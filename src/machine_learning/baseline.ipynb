{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Suppress tensorflow logging due to un-relevant warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = 19304\n",
    "validation_split = 4826\n",
    "test_split = 2682 \n",
    "\n",
    "asset_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML-Assets'))\n",
    "df = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "df.index = pd.to_datetime(df['Date'], format='%Y%m%d%H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Prediction (t + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: isbn = {1492078190}, title = {AI and Machine Learning for Coders: A Programmer's Guide to Artificial Intelligence}, author = {Moroney, Laurence}, Page(s): 169-171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = 1\n",
    "\n",
    "test_df = df[['Point_1_N_mean']][(training_split + validation_split):]\n",
    "naive_df = df[['Point_1_N_mean']][((training_split + validation_split) - delay):-delay]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(naive_df.index, df[(training_split + validation_split):]['Point_1_N_mean'], label='Test Dataset')\n",
    "plt.plot(naive_df.index, naive_df['Point_1_N_mean'], label='Naive Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Point 1 N mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(test_df['Point_1_N_mean'], naive_df['Point_1_N_mean']).numpy()}\")\n",
    "print(f\"MAE: {mean_absolute_error(test_df['Point_1_N_mean'], naive_df['Point_1_N_mean']).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sckit-learn linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_df = df[(training_split + validation_split):]\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = lin_reg_df[['Average_Global_Radiation_(1h)', 'PT100_Temperature_mean']]\n",
    "y = lin_reg_df['Point_1_N_mean']\n",
    "\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lin_reg_df.index, y, label='Test Dataset')\n",
    "plt.plot(lin_reg_df.index, predictions, label='Linear Regression Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y, predictions)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24\n",
    "\n",
    "test_df = df[(training_split + validation_split):][['Point_1_N_mean']].dropna()\n",
    "moving_avg = df[(training_split + validation_split):][['Point_1_N_mean']].rolling(window=window).mean().dropna()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_df.index, test_df['Point_1_N_mean'], label='Test Dataset')\n",
    "plt.plot(moving_avg.index, moving_avg, label='Moving Average')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Point 1 N mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(test_df['Point_1_N_mean'][window-1:], moving_avg['Point_1_N_mean'].to_numpy())}\")\n",
    "print(f\"MAE: {mean_absolute_error(test_df['Point_1_N_mean'][window-1:], moving_avg['Point_1_N_mean'].to_numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html#statsmodels.tsa.seasonal.seasonal_decompose\n",
    "\n",
    "(Copilot-generated description!!!) Statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24\n",
    "\n",
    "test_df = df[(training_split + validation_split):][['Point_1_N_mean']]\n",
    "seasonal_df = seasonal_decompose(test_df, model='additive', period=window)\n",
    "seasonal_df.plot()\n",
    "\n",
    "# Extract the trend component from the seasonal decomposition\n",
    "seasional_trend_df = pd.DataFrame(seasonal_df.trend)\n",
    "\n",
    "# Drop NaN values from both series\n",
    "test_dropped_df = test_df.dropna()\n",
    "seasional_trend_dropped_df = seasional_trend_df.dropna()\n",
    "test_truncated = test_dropped_df.iloc[:len(seasional_trend_dropped_df)]\n",
    "\n",
    "# Plot the original data, truncated data, and the trend component\n",
    "plt.figure(figsize=(10, 5))\n",
    "#plt.plot(test_dropped_df.index, test_dropped_df['Point_1_N_mean'], label='Original Data', linestyle='--')\n",
    "plt.plot(test_truncated.index, test_truncated['Point_1_N_mean'], label='Truncated Data')\n",
    "plt.plot(seasional_trend_dropped_df.index, seasional_trend_dropped_df['trend'], label='Trend Component')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Point 1 N mean')\n",
    "plt.legend()\n",
    "plt.title('Original Data vs. Trend Component vs. Truncated Data')\n",
    "plt.show()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(test_truncated['Point_1_N_mean'], seasional_trend_dropped_df['trend'])}\")\n",
    "print(f\"MAE: {mean_absolute_error(test_truncated['Point_1_N_mean'], seasional_trend_dropped_df['trend'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented Dickey-Fuller test + KPSS test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(GitHub Copilot) {\n",
    "The KPSS and ADF tests should be applied to the dataset that you are using to train your machine learning model. This is because the purpose of these tests is to help you understand the underlying properties of your data so that you can appropriately preprocess the data and select a suitable model. If your training data is non-stationary, the model you train on this data may not perform well on the test data, even if the test data is from the same non-stationary distribution. This is because many machine learning models, especially linear models and time series models, assume that the data is stationary. Applying the tests to the test set would not be as useful, because you do not use the test set to train your model. The test set is used to evaluate the performance of the model that was trained on the training set. However, if the test set is from a different distribution than the training set, this could indicate a problem with your data splitting method. In summary, apply the KPSS and ADF tests to your training data to check for stationarity before training your model. If the tests indicate that your data is non-stationary, you may need to transform your data to make it stationary before training your model.}\n",
    "\n",
    "Relevant Article: https://medium.com/@tannyasharma21/comparision-study-of-adf-vs-kpss-test-c9d8dec4f62a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/\n",
    "\n",
    "The Augmented Dickey-Fuller Test is used to determine if time-series data is stationary or not. Similar to a t-test, we set a significance level before the test and make conclusions on the hypothesis based on the resulting p-value. \"Another point to remember is the ADF test is fundamentally a statistical significance test\" - Selva Prabhakaran (https://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.machinelearningplus.com/time-series/kpss-test-for-stationarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Info!) This cell is heavily influenced by GitHub Copilot\n",
    "\n",
    "# ADF Test\n",
    "print('Results of Augmented Dickey-Fuller Test (ADF):')\n",
    "dftest_adf = adfuller(df[:(validation_split + test_split)]['Point_1_N_mean'], autolag='AIC')  # Choosing data that ML model has seen (Excluding test data)\n",
    "\n",
    "# Create a Series to hold the ADF test results\n",
    "adf_output = pd.Series(dftest_adf[0:4], index=['Test Statistic (ADF)', 'p-value (ADF)', '#Lags Used (ADF)', 'Number of Observations Used (ADF)'])\n",
    "\n",
    "# Add critical values to the Series\n",
    "for key, value in dftest_adf[4].items():\n",
    "    adf_output[f'Critical Value ({key}) (ADF)'] = value\n",
    "\n",
    "print(adf_output)\n",
    "\n",
    "# Determine if we reject the null hypothesis based on a significance level of 0.05\n",
    "if adf_output['p-value (ADF)'] < 0.05:\n",
    "    print(\"Reject the null hypothesis (H0) - Data is stationary (ADF)\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0) - Data is non-stationary (ADF)\")\n",
    "\n",
    "# KPSS Test\n",
    "print('\\nResults of KPSS Test:')\n",
    "kpsstest = kpss(df[:(validation_split + test_split)]['Point_1_N_mean'], regression='ct', nlags='auto')\n",
    "\n",
    "# Create a Series to hold the KPSS test results\n",
    "kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic (KPSS)', 'p-value (KPSS)', 'Lags Used (KPSS)'])\n",
    "\n",
    "# Add critical values to the Series\n",
    "for key, value in kpsstest[3].items():\n",
    "    kpss_output[f'Critical Value ({key}) (KPSS)'] = value\n",
    "\n",
    "print(kpss_output)\n",
    "\n",
    "# Determine if we reject the null hypothesis based on a significance level of 0.05\n",
    "if kpss_output['p-value (KPSS)'] < 0.05:\n",
    "    print(\"Reject the null hypothesis (H0) - Data is non-stationary (KPSS)\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0) - Data is stationary (KPSS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Info!) This cell is heavily influenced by GitHub Copilot\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "seasonal_df = seasonal_decompose(df[:(validation_split + test_split)]['Point_1_N_mean'], model='additive', period=window)\n",
    "residuals = df[:(validation_split + test_split)]['Point_1_N_mean'] - seasonal_df.trend  # Extract residuals\n",
    "\n",
    "# ADF Test on residuals\n",
    "print('Results of Augmented Dickey-Fuller Test (ADF) on Residuals:')\n",
    "dftest_adf_resid = adfuller(residuals.dropna(), autolag='AIC')  # Drop NaN values from residuals before testing\n",
    "\n",
    "# Create a Series to hold the ADF test results for residuals\n",
    "adf_output_resid = pd.Series(dftest_adf_resid[0:4], index=['Test Statistic (ADF Residuals)', 'p-value (ADF Residuals)', '#Lags Used (ADF Residuals)', 'Number of Observations Used (ADF Residuals)'])\n",
    "\n",
    "# Add critical values to the Series\n",
    "for key, value in dftest_adf_resid[4].items():\n",
    "    adf_output_resid[f'Critical Value ({key}) (ADF Residuals)'] = value\n",
    "\n",
    "print(adf_output_resid)\n",
    "\n",
    "# Determine if we reject the null hypothesis based on a significance level of 0.05\n",
    "if adf_output_resid['p-value (ADF Residuals)'] < 0.05:\n",
    "    print(\"Reject the null hypothesis (H0) - Data is stationary (ADF Residuals)\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0) - Data is non-stationary (ADF Residuals)\")\n",
    "\n",
    "# KPSS Test on residuals\n",
    "print('\\nResults of KPSS Test on Residuals:')\n",
    "kpsstest_resid = kpss(residuals.dropna(), regression='ct', nlags='auto')  # Drop NaN values from residuals before testing\n",
    "\n",
    "# Create a Series to hold the KPSS test results for residuals\n",
    "kpss_output_resid = pd.Series(kpsstest_resid[0:3], index=['Test Statistic (KPSS Residuals)', 'p-value (KPSS Residuals)', 'Lags Used (KPSS Residuals)'])\n",
    "\n",
    "# Add critical values to the Series\n",
    "for key, value in kpsstest_resid[3].items():\n",
    "    kpss_output_resid[f'Critical Value ({key}) (KPSS Residuals)'] = value\n",
    "\n",
    "print(kpss_output_resid)\n",
    "\n",
    "# Determine if we reject the null hypothesis based on a significance level of 0.05\n",
    "if kpss_output_resid['p-value (KPSS Residuals)'] < 0.05:\n",
    "    print(\"Reject the null hypothesis (H0) - Data is non-stationary (KPSS Residuals)\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0) - Data is stationary (KPSS Residuals)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX  (Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX\n",
    "\n",
    "https://towardsdatascience.com/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6\n",
    "\n",
    "https://pypi.org/project/pmdarima/\n",
    "\n",
    "(Copilot-generated description!!!) SARIMAX is an acronym for Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model. It is a class of models that explains a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=15000 # Varaible to shift how big of training+vaildation set we want to use\n",
    "\n",
    "data = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "data.index = pd.to_datetime(data['Date'], format='%Y%m%d%H').dropna()\n",
    "data = data[['Point_1_N_mean', 'Average_Global_Radiation_(1h)','PT100_Temperature_mean']][a:-test_split]\n",
    "data['month'] = data.index.month\n",
    "data['hour'] = data.index.hour\n",
    "data['day'] = data.index.dayofyear\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(\n",
    "    data['Point_1_N_mean'],  # The time series to which the ARIMA model is fit.\n",
    "    exogenous=data[['month', 'hour', 'day', 'Average_Global_Radiation_(1h)', 'PT100_Temperature_mean']],  # The external variables to include in the model.\n",
    "    start_p=1,  # This is the order of the AR term.\n",
    "    start_q=1,  # This is the order of the MA term.\n",
    "    test='adf',  # The type of unit root test to use in order to determine the order of differencing.\n",
    "    max_p=3,  # The maximum value of p to try when fitting the model.\n",
    "    max_q=3,  # The maximum value of q to try when fitting the model.\n",
    "    m=7,  # The number of periods in each season. This affects the seasonal differencing.\n",
    "    start_P=0,  # The starting value of P in auto_arima. This is the order of the seasonal AR term.\n",
    "    seasonal=True,  # Whether to include seasonal differencing in the model.\n",
    "    d=None,  # The order of first-differencing. If None, the value is automatically determined.\n",
    "    D=1,  # The order of seasonal differencing.\n",
    "    trace=True,  # Whether to print status on the fits.\n",
    "    error_action='ignore',  # If a model cannot be fit, ignore the error and continue.\n",
    "    suppress_warnings=True,  # If True, do not print warnings.\n",
    "    stepwise=True,  # If True, use the stepwise algorithm to fit the model.\n",
    "    maxiter=15  # The maximum number of function evaluations.\n",
    ")\n",
    "\n",
    "model.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code is reused from the following code: https://gist.githubusercontent.com/brendanartley/c69185f28e678c7221546a9c43825ec0/raw/d702ee2d1e4ad57e4ea9f92e5fa09a3414b509e8/gistfile1.txt\n",
    "Reference: https://towardsdatascience.com/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, periods=test_split): # , d = df[(training_split + validation_split):])\n",
    "    # Forecast\n",
    "    forecast, conf_int = model.predict(n_periods=periods, return_conf_int=True)\n",
    "    \n",
    "    # Generate date range for forecast\n",
    "    forecast_index = pd.date_range(start=data.index[-1], periods=periods + 1, freq='H')[1:]\n",
    "   \n",
    "    # Define start and end indices for plotting\n",
    "    lookback_variable = 500*3\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df.index[(training_split+validation_split)-lookback_variable:-test_split], df['Point_1_N_mean'].iloc[(training_split+validation_split)-lookback_variable:-test_split], label='Training and Validation Dataset', color='black')\n",
    "    plt.plot(df[-test_split:].index, df['Point_1_N_mean'][-test_split:])\n",
    "    plt.plot(forecast_index, forecast, label='SARIMAx Forecast')\n",
    "    plt.fill_between(forecast_index, conf_int[:, 0], conf_int[:, 1], color='k', alpha=0.2) # Between the confidence intervals\n",
    "    plt.title(\"SARIMAx Forecast\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Point_1_N_mean\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    forecast_values = forecast.to_numpy().flatten()\n",
    "    test_values = test_df.to_numpy().flatten()\n",
    "   \n",
    "    print(f\"MSE: {mean_squared_error(test_values, forecast_values)}\")\n",
    "    print(f\"MAE: {mean_absolute_error(test_values, forecast_values)}\")\n",
    "\n",
    "forecast(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
