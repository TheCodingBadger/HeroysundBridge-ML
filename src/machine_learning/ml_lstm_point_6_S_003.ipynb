{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_toolkit import df_to_X_y\n",
    "from ml_toolkit import plot_predictions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date                | Point_1_N_mean | PT100_Temperature_mean | Average_Global_Radiation_(1h) | Day sin          | Day cos          | Year sin       | Year cos       |\n",
    "|---------------------|----------------|------------------------|-------------------------------|------------------|------------------|----------------|----------------|\n",
    "| 2020-09-02 12:00:00 | 18.324186      | 16.261441              | 954.0                         | 7.392965e-12     | -1.000000e+00    | -0.961130      | -0.276097      |\n",
    "| 2020-09-03 12:00:00 | 24.667922      | 16.858222              | 966.0                         | 3.094426e-12     | -1.000000e+00    | -0.965740      | -0.259512      |\n",
    "| 2020-09-04 10:00:00 | -15.535099     | 14.256647              | 131.6                         | 5.000000e-01     | -8.660254e-01    | -0.969715      | -0.244241      |\n",
    "| 2020-09-04 14:00:00 | -10.819921     | 14.687477              | 314.0                         | -5.000000e-01    | -8.660254e-01    | -0.970411      | -0.241458      |\n",
    "| 2020-09-04 18:00:00 | -15.399971     | 13.874647              | 236.0                         | -1.000000e+00    | -1.359232e-12    | -0.971100      | -0.238673      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML'))\n",
    "asset_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML-Assets'))\n",
    "\n",
    "print(\"Path to git folder:\", git_folder)\n",
    "print(\"Path to asset folder:\", asset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opening file: \\silver\\combined with relevant columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "df.index = pd.to_datetime(df['Date'], format='%Y%m%d%H')\n",
    "df.to_csv(os.path.join(asset_folder, 'silver','inspection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding relevant time/dates columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = df[['Point_6_S_mean', 'PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]['2020-10-01 00:00:00':]#\n",
    "model_dataset['seconds'] = model_dataset.index.map(pd.Timestamp.timestamp)\n",
    "model_dataset['Day sin'] = np.sin(model_dataset['seconds'] * (2 * np.pi / 86400))\n",
    "model_dataset['Day cos'] = np.cos(model_dataset['seconds'] * (2 * np.pi / 86400))\n",
    "model_dataset['Year sin'] = np.sin(model_dataset['seconds'] * (2 * np.pi / 31536000))\n",
    "model_dataset['Year cos'] = np.cos(model_dataset['seconds'] * (2 * np.pi / 31536000))\n",
    "model_dataset['days_since_start'] = (model_dataset.index - model_dataset.index[0]).days\n",
    "model_dataset.drop(columns=['seconds'], inplace=True)\n",
    "display(model_dataset)\n",
    "plt.plot(model_dataset.index, model_dataset['Point_6_S_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-, val- and testing split +++ Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(model_dataset)\n",
    "training_split = int(0.72 * dataset_size)\n",
    "validation_split = int(0.18 * dataset_size)\n",
    "test_split = dataset_size - training_split - validation_split\n",
    "print(f\"Training split: {training_split}, Validation split: {validation_split}, Test split: {test_split}\")\n",
    "assert training_split + validation_split + test_split == dataset_size, \"Invalid split sizes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_dataset[:training_split])\n",
    "print(\"-------------------\")\n",
    "print(model_dataset[training_split:-test_split])\n",
    "print(\"-------------------\")\n",
    "print(model_dataset[-test_split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assigning labels and targets - x- and y- train,-val and -test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 24*30\n",
    "\n",
    "X, y = df_to_X_y(model_dataset, window_size)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Splitting the data into training, validation and test sets\n",
    "X_train, y_train = X[:training_split], y[:training_split]\n",
    "X_valid, y_valid = X[training_split:(training_split + validation_split)], y[training_split:(training_split + validation_split)]\n",
    "X_test, y_test = X[-test_split:], y[-test_split:]\n",
    "print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normlaization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 2D (Normalisation requires 2D input)\n",
    "X_train_2D = np.reshape(X_train, (X_train.shape[0] * X_train.shape[1], X_train.shape[2]))\n",
    "X_valid_2D = np.reshape(X_valid, (X_valid.shape[0] * X_valid.shape[1], X_valid.shape[2]))\n",
    "X_test_2D = np.reshape(X_test, (X_test.shape[0] * X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Apply scaling\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled_2D = scaler_x.fit_transform(X_train_2D)\n",
    "X_valid_scaled_2D = scaler_x.transform(X_valid_2D)\n",
    "X_test_scaled_2D = scaler_x.transform(X_test_2D)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train_scaled = np.reshape(X_train_scaled_2D, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_valid_scaled = np.reshape(X_valid_scaled_2D, (X_valid.shape[0], X_valid.shape[1], X_valid.shape[2]))\n",
    "X_test_scaled = np.reshape(X_test_scaled_2D, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_valid_scaled = scaler_y.transform(y_valid.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_model_point_6_S_004'\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "learning_rate = 0.001\n",
    "patience = 12\n",
    "verbose = 1\n",
    "saving_frequency = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(InputLayer((window_size, (len(model_dataset.columns)-1))))\n",
    "# model.add(LSTM(64, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "# model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.5))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_file_path = os.path.join(asset_folder, 'gold', model_name)\n",
    "model = load_model(model_file_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint(os.path.join(asset_folder, 'gold', str(model_name)), save_best_only=True, mode='auto', verbose=verbose)\n",
    "es = EarlyStopping(monitor='val_loss', patience=patience, mode='auto', verbose=verbose)\n",
    "model.compile(optimizer=Adam(learning_rate), loss='mse', metrics=['mae'])\n",
    "history = model.fit(X_train_scaled, y_train_scaled, validation_data=(X_valid_scaled, y_valid_scaled), epochs=epochs, batch_size=batch_size, callbacks=[es,cp], verbose=verbose) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of ML-metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get the number of epochs\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Get the index of the best model\n",
    "best_model_index = np.argmin(val_loss)\n",
    "\n",
    "# Plot the training loss vs validation loss\n",
    "plt.plot(epochs, train_loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.scatter(best_model_index + 1, val_loss[best_model_index], color='g', label='Best Model')\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.xlabel('Epochs',fontsize=16)\n",
    "plt.ylabel('Loss [MSE]',fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "# Display the MAE value of the best model\n",
    "best_model_mae = val_loss[best_model_index]\n",
    "plt.annotate(f'MAE: {best_model_mae:.4f}', (best_model_index + 1, best_model_mae), xytext=(10, 10),\n",
    "             textcoords='offset points', color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as score\n",
    "\n",
    "model_file_path = os.path.join(asset_folder, 'gold', model_name)\n",
    "model = load_model(model_file_path)\n",
    "plot_predictions(model_dataset, test_split, model, X_test_scaled, y_test, start=0, end=test_split, scaler=scaler_y)\n",
    "#print(f\"R^2 Score: {score(X_test_scaled, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
