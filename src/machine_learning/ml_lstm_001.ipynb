{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_toolkit import df_to_X_y\n",
    "from ml_toolkit import plot_predictions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date                | Point_1_N_mean | PT100_Temperature_mean | Average_Global_Radiation_(1h) | hour | date | year |\n",
    "|---------------------|----------------|------------------------|-------------------------------|------|------|------|\n",
    "| 2020-09-02 12:00:00 | 18.324186      | 16.261441              | 954.0                         | 12   | 246  | 2020 |\n",
    "| 2020-09-03 12:00:00 | 24.667922      | 16.858222              | 966.0                         | 12   | 247  | 2020 |\n",
    "| 2020-09-04 10:00:00 | -15.535099     | 14.256647              | 131.6                         | 10   | 248  | 2020 |\n",
    "| 2020-09-04 14:00:00 | -10.819921     | 14.687477              | 314.0                         | 14   | 248  | 2020 |\n",
    "| 2020-09-04 18:00:00 | -15.399971     | 13.874647              | 236.0                         | 18   | 248  | 2020 |\n",
    "| ...                 | ...            | ...                    | ...                           | ...  | ...  | ...  |\n",
    "| 2023-12-31 18:00:00 | -20.227229     | -4.738139              | -3.0                          | 18   | 365  | 2023 |\n",
    "| 2023-12-31 19:00:00 | -19.488729     | -4.625233              | -3.0                          | 19   | 365  | 2023 |\n",
    "| 2023-12-31 20:00:00 | -21.503484     | -4.645648              | -2.9                          | 20   | 365  | 2023 |\n",
    "| 2023-12-31 21:00:00 | -22.667015     | -4.756255              | -2.9                          | 21   | 365  | 2023 |\n",
    "| 2023-12-31 22:00:00 | -22.800138     | -4.433085              | -3.2                          | 22   | 365  | 2023 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML'))\n",
    "asset_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML-Assets'))\n",
    "\n",
    "print(\"Path to git folder:\", git_folder)\n",
    "print(\"Path to asset folder:\", asset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opening file: \\silver\\combined with relevant columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "df.index = pd.to_datetime(df['Date'], format='%Y%m%d%H')\n",
    "df.to_csv(os.path.join(asset_folder, 'silver','inspection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding relevant time/dates columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = df[['Point_1_N_mean', 'PT100_Temperature_mean', 'Average_Global_Radiation_(1h)']]\n",
    "model_dataset.loc[:, 'time_unit'] = pd.to_datetime(df['Date'], format='%Y%m%d%H')\n",
    "\n",
    "model_dataset['hour'] = model_dataset['time_unit'].dt.hour\n",
    "model_dataset['date'] = model_dataset['time_unit'].dt.dayofyear\n",
    "model_dataset['year'] = model_dataset['time_unit'].dt.year\n",
    "\n",
    "model_dataset.drop(columns=['time_unit'], inplace=True)  # Drop the temporary column if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-, val- and testing split +++ Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 24\n",
    "\n",
    "dataset_size = len(model_dataset)\n",
    "training_split = int(0.72 * dataset_size)\n",
    "validation_split = int(0.18 * dataset_size)\n",
    "test_split = dataset_size - training_split - validation_split\n",
    "print(f\"Training split: {training_split}, Validation split: {validation_split}, Test split: {test_split}\")\n",
    "assert training_split + validation_split + test_split == dataset_size, \"Invalid split sizes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assigning labels and targets - x- and y- train,-val and -test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_to_X_y(model_dataset, window_size)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Splitting the data into training, validation and test sets\n",
    "X_train, y_train = X[:training_split], y[:training_split]\n",
    "X_valid, y_valid = X[training_split:(training_split + validation_split)], y[training_split:(training_split + validation_split)]\n",
    "X_test, y_test = X[-test_split:], y[-test_split:]\n",
    "print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normlaization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 2D (Normalisation requires 2D input)\n",
    "X_train_2D = np.reshape(X_train, (X_train.shape[0] * X_train.shape[1], X_train.shape[2]))\n",
    "X_valid_2D = np.reshape(X_valid, (X_valid.shape[0] * X_valid.shape[1], X_valid.shape[2]))\n",
    "X_test_2D = np.reshape(X_test, (X_test.shape[0] * X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Apply scaling\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled_2D = scaler_x.fit_transform(X_train_2D)\n",
    "X_valid_scaled_2D = scaler_x.transform(X_valid_2D)\n",
    "X_test_scaled_2D = scaler_x.transform(X_test_2D)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train_scaled = np.reshape(X_train_scaled_2D, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_valid_scaled = np.reshape(X_valid_scaled_2D, (X_valid.shape[0], X_valid.shape[1], X_valid.shape[2]))\n",
    "X_test_scaled = np.reshape(X_test_scaled_2D, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_valid_scaled = scaler_y.transform(y_valid.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_model_1'\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 10\n",
    "verbose = 0\n",
    "saving_frequency = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer((window_size, (len(model_dataset.columns)-1))))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint(os.path.join(asset_folder, 'gold', str(model_name)), save_best_only=True, mode='auto', verbose=verbose)\n",
    "es = EarlyStopping(monitor='val_loss', patience=patience, mode='auto', verbose=verbose)\n",
    "model.compile(optimizer=Adam(learning_rate), loss=mean_absolute_error, metrics=['mae'])\n",
    "history = model.fit(X_train_scaled, y_train_scaled, validation_data=(X_valid_scaled, y_valid_scaled), epochs=epochs, batch_size=batch_size, callbacks=[es,cp], verbose=verbose) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization of ML-metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get the number of epochs\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Get the index of the best model\n",
    "best_model_index = np.argmin(val_loss)\n",
    "\n",
    "# Plot the training loss vs validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.scatter(best_model_index + 1, val_loss[best_model_index], color='g', label='Best Model')\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Display the MAE value of the best model\n",
    "best_model_mae = val_loss[best_model_index]\n",
    "plt.annotate(f'MAE: {best_model_mae:.4f}', (best_model_index + 1, best_model_mae), xytext=(10, 10),\n",
    "             textcoords='offset points', color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = os.path.join(asset_folder, 'gold', model_name)\n",
    "model = load_model(model_file_path)\n",
    "plot_predictions(model_dataset, test_split, model, X_test_scaled, y_test, start=0, end=test_split, scaler=scaler_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
