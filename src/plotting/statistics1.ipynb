{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# point1n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML'))\n",
    "asset_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML-Assets'))\n",
    "\n",
    "print(\"Path to git folder:\", git_folder)\n",
    "print(\"Path to asset folder:\", asset_folder)\n",
    "\n",
    "df = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "df.index = pd.to_datetime(df['Date'], format='%Y%m%d%H')\n",
    "df = df[['Point_1_N_mean', 'PT100_Temperature_mean']]\n",
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_means = df.groupby(['Year', 'Month']).mean().reset_index()\n",
    "display(monthly_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(monthly_means)\n",
    "\n",
    "# Convert Year-Month to datetime format for plotting\n",
    "df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Sort the DataFrame by date for plotting\n",
    "df.sort_values('Date', inplace=True)\n",
    "\n",
    "# Perform linear regression on Point_1_N_mean\n",
    "slope_p1, intercept_p1, _, _, _ = linregress(df.index, df['Point_1_N_mean'])\n",
    "regression_line_p1 = slope_p1 * df.index + intercept_p1\n",
    "\n",
    "# Perform linear regression on PT100_Temperature_mean\n",
    "slope_temp, intercept_temp, _, _, _ = linregress(df.index, df['PT100_Temperature_mean'])\n",
    "regression_line_temp = slope_temp * df.index + intercept_temp\n",
    "\n",
    "# Plotting trends for Point_1_N_mean and PT100_Temperature_mean with regression lines\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.plot(df['Date'], df['Point_1_N_mean'], label='Point_1_N_mean')\n",
    "plt.plot(df['Date'], regression_line_p1, label=f'Point_1_N_mean Regression (slope={slope_p1:.2f})')\n",
    "plt.plot(df['Date'], df['PT100_Temperature_mean'], label='PT100_Temperature_mean')\n",
    "plt.plot(df['Date'], regression_line_temp, label=f'PT100_Temperature_mean Regression (slope={slope_temp:.2f})')\n",
    "plt.xlabel('Date', fontsize=16*1.5)\n",
    "plt.ylabel(\"'Point_1_S_mean' Mean Strain [μm/m]\", fontsize=16*1.5)\n",
    "#plt.title('Trend Over Time with Linear Regression', fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14*1.5)\n",
    "plt.xticks(fontsize=14*1.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# point6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML'))\n",
    "asset_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML-Assets'))\n",
    "\n",
    "print(\"Path to git folder:\", git_folder)\n",
    "print(\"Path to asset folder:\", asset_folder)\n",
    "\n",
    "df = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "df.index = pd.to_datetime(df['Date'], format='%Y%m%d%H')\n",
    "df = df[['Point_6_S_mean', 'PT100_Temperature_mean']]\n",
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_means = df.groupby(['Year', 'Month']).mean().reset_index()\n",
    "display(monthly_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(monthly_means)\n",
    "\n",
    "# Convert Year-Month to datetime format for plotting\n",
    "df['Date'] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Sort the DataFrame by date for plotting\n",
    "df.sort_values('Date', inplace=True)\n",
    "\n",
    "# Perform linear regression on Point_1_N_mean\n",
    "slope_p1, intercept_p1, _, _, _ = linregress(df.index, df['Point_6_S_mean'])\n",
    "regression_line_p1 = slope_p1 * df.index + intercept_p1\n",
    "\n",
    "# Perform linear regression on PT100_Temperature_mean\n",
    "slope_temp, intercept_temp, _, _, _ = linregress(df.index, df['PT100_Temperature_mean'])\n",
    "regression_line_temp = slope_temp * df.index + intercept_temp\n",
    "\n",
    "# Plotting trends for Point_1_N_mean and PT100_Temperature_mean with regression lines\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.plot(df['Date'], df['Point_6_S_mean'], label='Point_6_S_mean')\n",
    "plt.plot(df['Date'], regression_line_p1, label=f'Point_6_S_mean Regression (slope={slope_p1:.2f})')\n",
    "plt.plot(df['Date'], df['PT100_Temperature_mean'], label='PT100_Temperature_mean')\n",
    "plt.plot(df['Date'], regression_line_temp, label=f'PT100_Temperature_mean Regression (slope={slope_temp:.2f})')\n",
    "plt.xlabel('Date', fontsize=16*1.5)\n",
    "plt.ylabel(\"'Point_6_S_mean' Mean Strain [μm/m]\", fontsize=16*1.5)\n",
    "#plt.title('Trend Over Time with Linear Regression', fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(fontsize=14*1.5)\n",
    "plt.xticks(fontsize=14*1.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspection of gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Calculate time intervals between consecutive indices\n",
    "time_diff = df.index.to_series().diff()\n",
    "\n",
    "# Check for gaps in the index\n",
    "gaps = time_diff[time_diff != pd.Timedelta(seconds=1)]\n",
    "\n",
    "# Print information about the gaps\n",
    "if not gaps.empty:\n",
    "    print(\"Gaps in index:\")\n",
    "    print(gaps)\n",
    "else:\n",
    "    print(\"No gaps found in the index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "time_diff = df.index.to_series().diff()\n",
    "\n",
    "# Define the threshold for gap detection (1 hour in this case)\n",
    "threshold = pd.Timedelta(hours=1)\n",
    "\n",
    "# Check for gaps in the index greater than the threshold\n",
    "gaps = time_diff[time_diff > threshold]\n",
    "\n",
    "# Count the number of gaps greater than 1 hour\n",
    "num_gaps_greater_than_1_hour = len(gaps[gaps > threshold])\n",
    "\n",
    "print(f\"Number of gaps greater than 1 hour: {num_gaps_greater_than_1_hour}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate time intervals between consecutive indices\n",
    "time_diff = df.index.to_series().diff()\n",
    "\n",
    "# Define the threshold for gap detection (1 hour in this case)\n",
    "threshold = pd.Timedelta(hours=1)\n",
    "\n",
    "# Check for gaps in the index greater than the threshold\n",
    "gaps = time_diff[time_diff > threshold]\n",
    "\n",
    "# Initialize a list to store gap information\n",
    "gap_info = []\n",
    "\n",
    "# Loop through gaps greater than the threshold\n",
    "for idx in gaps.index:\n",
    "    diff = gaps[idx]\n",
    "    if diff > threshold:\n",
    "        start_gap = idx - diff  # Start of the gap\n",
    "        gap_info.append((start_gap, idx, diff))\n",
    "\n",
    "# Print information about the gaps\n",
    "if gap_info:\n",
    "    print(\"Information about the gaps greater than 1 hour:\")\n",
    "    for start, end, length in gap_info:\n",
    "        print(f\"Gap from {start} to {end}, length: {length} hours\")\n",
    "else:\n",
    "    print(\"No gaps found in the index greater than 1 hour.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slutningstatisikk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score as score\n",
    "\n",
    "def df_to_X_y(dataset, window_size = 6):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(window_size, len(dataset)):\n",
    "        X.append(dataset.iloc[i-window_size:i, 1:].values)  # Use all columns except 'Point_1_N_mean' as input\n",
    "        y.append(dataset.iloc[i, 0])  # Use 'Point_1_N_mean' as output\n",
    "    a = np.array(X)\n",
    "    b= np.array(y)\n",
    "    return a, b\n",
    "\n",
    "# Plot the predictions of the model\n",
    "def plot_predictions(dataset, split, model, X, y, start=0, end=100, scaler=None):\n",
    "    predictions = model.predict(X).flatten()\n",
    "    predictions = predictions.reshape(-1, 1) \n",
    "    if scaler is not None:\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "    df = pd.DataFrame(data={'Predictions': predictions.flatten(),'Actuals': y})\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(dataset[-split:].index,df['Actuals'][start:end], label='Test Dataset')\n",
    "    plt.plot(dataset[-split:].index,df['Predictions'][start:end], label='ML Predictions')\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.xlabel('Date',fontsize=16)\n",
    "    plt.ylabel(\"'Point_1_N_mean' Strain [μm/m]\",fontsize=16)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.show()\n",
    "    print(f\"MSE: {mean_squared_error(df['Actuals'][start:end], df['Predictions'][start:end])}\")\n",
    "    print(f\"MAE: {mean_absolute_error(df['Actuals'][start:end], df['Predictions'][start:end])}\")\n",
    "    print(f\"R^2 Score: {score(df['Actuals'][start:end], df['Predictions'][start:end])}\")\n",
    "    display(df)\n",
    "\n",
    "\n",
    "# Plot on validation set\n",
    "def plot_predictions_1(dataset, split, model, X, y, start=0, end=100, scaler=None):\n",
    "    predictions = model.predict(X).flatten()\n",
    "    predictions = predictions.reshape(-1, 1) \n",
    "    if scaler is not None:\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "    df = pd.DataFrame(data={'Predictions': predictions.flatten(),'Actuals': y})\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(dataset[split[0]:-split[2]].index,df['Actuals'][start:end])\n",
    "    plt.plot(dataset[split[0]:-split[2]].index,df['Predictions'][start:end])\n",
    "    plt.show()\n",
    "    print(f\"MSE: {mean_squared_error(df['Actuals'][start:end], df['Predictions'][start:end])}\")\n",
    "    print(f\"MAE: {mean_absolute_error(df['Actuals'][start:end], df['Predictions'][start:end])}\")\n",
    "    print(f\"R^2 Score: {score(df['Actuals'][start:end], df['Predictions'][start:end])}\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML'))\n",
    "asset_folder = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', 'HeroysundBridge-ML-Assets'))\n",
    "\n",
    "print(\"Path to git folder:\", git_folder)\n",
    "print(\"Path to asset folder:\", asset_folder)\n",
    "\n",
    "df = pd.read_parquet(os.path.join(asset_folder, 'silver','combined_data_v01.parquet'))\n",
    "df.index = pd.to_datetime(df['Date'], format='%Y%m%d%H')\n",
    "df.to_csv(os.path.join(asset_folder, 'silver','inspection.csv'))\n",
    "\n",
    "model_dataset = df[['Point_6_S_mean', 'PT100_Temperature_mean', 'Average_Global_Radiation_(1h)', 'Relative_Humidity(1h)', 'Precipitation_(1h)', 'Average_of_Mean_Wind_(1h)']]['2020-10-01 00:00:00':]#\n",
    "model_dataset['seconds'] = model_dataset.index.map(pd.Timestamp.timestamp)\n",
    "model_dataset['Day sin'] = np.sin(model_dataset['seconds'] * (2 * np.pi / 86400))\n",
    "model_dataset['Day cos'] = np.cos(model_dataset['seconds'] * (2 * np.pi / 86400))\n",
    "model_dataset['Year sin'] = np.sin(model_dataset['seconds'] * (2 * np.pi / 31536000))\n",
    "model_dataset['Year cos'] = np.cos(model_dataset['seconds'] * (2 * np.pi / 31536000))\n",
    "#model_dataset['days_since_start'] = (model_dataset.index - model_dataset.index[0]).days\n",
    "model_dataset.drop(columns=['seconds'], inplace=True)\n",
    "display(model_dataset)\n",
    "plt.plot(model_dataset.index, model_dataset['Point_6_S_mean'])\n",
    "\n",
    "dataset_size = len(model_dataset)\n",
    "training_split = int(0.72 * dataset_size)\n",
    "validation_split = int(0.18 * dataset_size)\n",
    "test_split = dataset_size - training_split - validation_split\n",
    "print(f\"Training split: {training_split}, Validation split: {validation_split}, Test split: {test_split}\")\n",
    "assert training_split + validation_split + test_split == dataset_size, \"Invalid split sizes\"\n",
    "\n",
    "window_size = 24*30\n",
    "\n",
    "X, y = df_to_X_y(model_dataset, window_size)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Splitting the data into training, validation and test sets\n",
    "X_train, y_train = X[:training_split], y[:training_split]\n",
    "X_valid, y_valid = X[training_split:(training_split + validation_split)], y[training_split:(training_split + validation_split)]\n",
    "X_test, y_test = X[-test_split:], y[-test_split:]\n",
    "print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Reshape to 2D (Normalisation requires 2D input)\n",
    "X_train_2D = np.reshape(X_train, (X_train.shape[0] * X_train.shape[1], X_train.shape[2]))\n",
    "X_valid_2D = np.reshape(X_valid, (X_valid.shape[0] * X_valid.shape[1], X_valid.shape[2]))\n",
    "X_test_2D = np.reshape(X_test, (X_test.shape[0] * X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Apply scaling\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled_2D = scaler_x.fit_transform(X_train_2D)\n",
    "X_valid_scaled_2D = scaler_x.transform(X_valid_2D)\n",
    "X_test_scaled_2D = scaler_x.transform(X_test_2D)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train_scaled = np.reshape(X_train_scaled_2D, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_valid_scaled = np.reshape(X_valid_scaled_2D, (X_valid.shape[0], X_valid.shape[1], X_valid.shape[2]))\n",
    "X_test_scaled = np.reshape(X_test_scaled_2D, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_valid_scaled = scaler_y.transform(y_valid.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_model_point_6_S_002'\n",
    "model_file_path = os.path.join(asset_folder, 'gold', model_name)\n",
    "model = load_model(model_file_path)\n",
    "\n",
    "predictions = model.predict(X_test_scaled).flatten()\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "predictions = scaler_y.inverse_transform(predictions) \n",
    "\n",
    "pred_df = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mesu = model_dataset[-test_split:]['Point_6_S_mean'].values\n",
    "predic = pred_df.flatten()\n",
    "print(mesu, \"################################\", predic)\n",
    "# Assuming you have a DataFrame called df\n",
    "df_1=pd.DataFrame(mesu)\n",
    "df_2=pd.DataFrame(predic)\n",
    "df_1.to_parquet('C:\\\\Users\\\\erlih\\\\repos\\\\HeroysundBridge-ML-Assets\\\\mesu.parquet')\n",
    "df_2.to_parquet('C:\\\\Users\\\\erlih\\\\repos\\\\HeroysundBridge-ML-Assets\\\\predic.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Pearson's Correlation test\n",
    "from scipy.stats import pearsonr\n",
    "data1 = mesu\n",
    "data2 = predic\n",
    "\n",
    "print('Pearsons correlation test')\n",
    "stat, p = pearsonr(data1, data2)\n",
    "print('Pearson correlation coefficient=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    " print('Probably independent')\n",
    "else:\n",
    " print('Probably dependent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Paired Student's t-test\n",
    "from scipy.stats import ttest_rel\n",
    "data1 = mesu\n",
    "data2 = predic\n",
    "\n",
    "print('Paired Student t-test')\n",
    "stat, p = ttest_rel(data1, data2)\n",
    "print( 'p=%.100f' % (p))\n",
    "if p > 0.05:\n",
    " print('Probably the same distribution')\n",
    "else:\n",
    " print('Probably different distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = np.mean((mesu - predic) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate the mean and standard deviation for predic\n",
    "diff = mesu - predic\n",
    "\n",
    "mean_predic = np.mean(diff)\n",
    "std_predic = np.std(diff)\n",
    "\n",
    "# Parameter for z-scores for one-tailed confidence intervals\n",
    "z_95 = norm.ppf(0.95)  # 95% CI, one-tailed\n",
    "z_99 = norm.ppf(0.99)  # 99% CI, one-tailed\n",
    "\n",
    "# Confidence interval bounds\n",
    "upper_95_predic = mean_predic + z_95 * std_predic\n",
    "upper_99_predic = mean_predic + z_99 * std_predic\n",
    "\n",
    "# Generate x values for plotting\n",
    "x = np.linspace(0, mean_predic + 3*std_predic, 1000)\n",
    "pdf = norm.pdf(x, mean_predic, std_predic)\n",
    "\n",
    "print(f\"95% Confidence Interval for 'predic': {0:.2f} to {upper_95_predic:.2f}\")\n",
    "print(f\"99% Confidence Interval for 'predic': {0:.2f} to {upper_99_predic:.2f}\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, pdf, 'b-', label=f'Normal Distribution\\nMean = {mean_predic:.2f}, Std Dev = {std_predic:.2f}' )\n",
    "\n",
    "# Highlight the 99.5% Confidence Interval\n",
    "plt.fill_between(x, 0, norm.pdf(x, mean_predic, std_predic), where=(x <= upper_99_predic),\n",
    "                 color='green', alpha=0.3,\n",
    "                 label=f'99% CI: {0} to {upper_99_predic:.2f}')\n",
    "\n",
    "# Highlight the 95% Confidence Interval\n",
    "plt.fill_between(x, 0, norm.pdf(x, mean_predic, std_predic), where=(x <= upper_95_predic),\n",
    "                 color='gray', alpha=0.5,\n",
    "                 label=f'95% CI: {0} to {upper_95_predic:.2f}')\n",
    "\n",
    "plt.xlabel('Strain [µm/m]', fontsize=16)\n",
    "plt.ylabel('Probability Density', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of predic\n",
    "mean_predic = np.mean(predic)\n",
    "std_predic = np.std(predic)\n",
    "\n",
    "# Confidence interval around the mean of predic\n",
    "ci_lower_95 = predic - z_95 * std_predic\n",
    "ci_upper_95 = predic + z_95 * std_predic\n",
    "ci_lower_99 = predic - z_99 * std_predic\n",
    "ci_upper_99 = predic + z_99 * std_predic\n",
    "\n",
    "# X values for plotting\n",
    "x_values = np.arange(len(predic))  # Assuming predic has a length\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_values, mesu, label='Measured Data (mesu)', marker='o', linestyle='-')\n",
    "plt.plot(x_values, predic, label='Predicted Data (predic)', marker='x', linestyle='-')\n",
    "\n",
    "# Plot 99% CI first with green\n",
    "plt.fill_between(x_values, ci_lower_99, ci_upper_99, color='green', alpha=0.2, label='99% Confidence Interval')\n",
    "\n",
    "# Plot 95% CI on top with yellow\n",
    "plt.fill_between(x_values, ci_lower_95, ci_upper_95, color='gray', alpha=0.5, label='95% Confidence Interval')\n",
    "\n",
    "plt.xlabel('Elapsed Hours', fontsize=16)\n",
    "plt.ylabel(\"'Point_6_S_mean' Strain [µm/m]\", fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
